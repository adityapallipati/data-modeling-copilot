{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Exploration of Data Modeling Copilot\n",
    "\n",
    "Here we will test the various components of this project locally, including, but not limited to the local models downloaded from huggingface, creation of a persistent vector store, and the creation of a knowledge graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Load Models\n",
    "MODEL_PATH = '../models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Validate Major Components**\n",
    "\n",
    "Now we will validate the pieces of our puzzle (this is a non-exhaustive list):\n",
    "\n",
    "1. Can we load the model locally?\n",
    "2. Do both the LLM (flat-t5-base) and the embedding model (all-mpnet-base-v2) 'work'? (They don't need to be perfect we will improve performance iteratively)\n",
    "3. Can we create a vector store?\n",
    "4. Can we create a knowledge graph?\n",
    "5. Can we have have these components working together within an API?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FLAN-T5 Basic Test ===\n",
      "Loading from: /Users/adityapallipati/Developer/work/data-modeling-copilot/notebooks/../models/flan-t5-base\n",
      "\n",
      "Loading tokenizer and model...\n",
      "✓ Model and tokenizer loaded successfully\n",
      "✓ Model moved to MPS device\n",
      "Device: mps:0\n",
      "\n",
      "Testing inference...\n",
      "\n",
      "Input:  Convert column name 'model_score' to standard format.\n",
      "Output: Model\n",
      "\n",
      "Input:  Transform 'dt_last_active' to proper column name.\n",
      "Output: 'dt_last_active'\n",
      "\n",
      "Input:  Standardize column name: 'cust_id'\n",
      "Output: 'cust_id'\n",
      "\n",
      "✓ Inference test completed\n"
     ]
    }
   ],
   "source": [
    "def test_flan_t5_basic():\n",
    "    \"\"\"\n",
    "    Basic test of FLAN-T5 model functionality.\n",
    "    Tests loading, tokenization, and simple inference.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer) if successful\n",
    "    \"\"\"\n",
    "    print(\"=== FLAN-T5 Basic Test ===\")\n",
    "    \n",
    "    # 1. Setup\n",
    "    MODEL_PATH = '../models/flan-t5-base'\n",
    "    model_dir = Path(MODEL_PATH)\n",
    "    print(f\"Loading from: {model_dir.absolute()}\")\n",
    "    \n",
    "    # 2. Load Model & Tokenizer\n",
    "    try:\n",
    "        print(\"\\nLoading tokenizer and model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(str(model_dir))\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(str(model_dir))\n",
    "        print(\"✓ Model and tokenizer loaded successfully\")\n",
    "        \n",
    "        # 3. Move to MPS if available\n",
    "        if torch.backends.mps.is_available():\n",
    "            model = model.to('mps')\n",
    "            print(\"✓ Model moved to MPS device\")\n",
    "            print(f\"Device: {next(model.parameters()).device}\")\n",
    "    except Exception as e:\n",
    "        print(f\"× Error in setup: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # 4. Test Inference\n",
    "    try:\n",
    "        print(\"\\nTesting inference...\")\n",
    "        \n",
    "        # Test cases\n",
    "        test_cases = [\n",
    "            \"Convert column name 'model_score' to standard format.\",\n",
    "            \"Transform 'dt_last_active' to proper column name.\",\n",
    "            \"Standardize column name: 'cust_id'\"\n",
    "        ]\n",
    "        \n",
    "        for test_input in test_cases:\n",
    "            print(f\"\\nInput:  {test_input}\")\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
    "            if torch.backends.mps.is_available():\n",
    "                inputs = {k: v.to('mps') for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=128,\n",
    "                    num_beams=1,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            \n",
    "            # Decode\n",
    "            result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"Output: {result}\")\n",
    "            \n",
    "        print(\"\\n✓ Inference test completed\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"× Error during inference: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Run test\n",
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer = test_flan_t5_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like flat-t5-base performs poorly, in an ideal world a simple API call to one of the big players in the LLM space like gemini, claude, or openai will have no problem with this task but the goal of this project is to work within a strict set of parameters, there may be security concerns with using an opensource tool and it's not just an individual creating software for their own personal use.\n",
    "\n",
    "We want to work with an LLM that is \"approved\" for use, possibly fine tune it for our usecase, and use tools that are available for a developer working within specific constraints. These constraints aren't neccessarily a bad thing as AI/ML Engineers, the task is to solve AI/ML Problems efficiently whether that is setting up the infrastructure of an model for a production deployment, utilizing these tools for real world usecases such as an internal knowledge base or fraud detection. Working with LLMs as a hobbyist there is a certain freedom to choose whatever tool without much thought into scalability, reliability, and observability. Thinking beyond a hobbyist's mindset and considering broader implications of using LLMs for an enterpise is a valuable skill to develop and one we will intend to with this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ../models/all-mpnet-base-v2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MPNet Basic Test ===\n",
      "Loading from: /Users/adityapallipati/Developer/work/data-modeling-copilot/notebooks/../models/all-mpnet-base-v2\n",
      "\n",
      "Loading MPNet model...\n",
      "✓ Model loaded successfully\n",
      "\n",
      "Testing embedding generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding dimensions:\n",
      "Number of test cases: 4\n",
      "Embedding shape: (4, 768)\n",
      "Vector dimension: 768\n",
      "\n",
      "Similarity matrix shape: (4, 4)\n",
      "\n",
      "Example similarities between first and other sentences:\n",
      "Similarity with sentence 2: 0.2011\n",
      "Similarity with sentence 3: 0.4276\n",
      "Similarity with sentence 4: 0.3893\n",
      "\n",
      "✓ Embedding test completed\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def test_mpnet_basic():\n",
    "    \"\"\"\n",
    "    Basic test of MPNet model functionality.\n",
    "    Tests loading and embedding generation with verification of output dimensions.\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer: embedding_model if successful\n",
    "    \"\"\"\n",
    "    print(\"=== MPNet Basic Test ===\")\n",
    "    \n",
    "    # 1. Setup\n",
    "    MODEL_PATH = '../models/all-mpnet-base-v2'\n",
    "    model_dir = Path(MODEL_PATH)\n",
    "    print(f\"Loading from: {model_dir.absolute()}\")\n",
    "    \n",
    "    # 2. Load Model\n",
    "    try:\n",
    "        print(\"\\nLoading MPNet model...\")\n",
    "        embedding_model = SentenceTransformer(str(model_dir))\n",
    "        print(\"✓ Model loaded successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"× Error loading model: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 3. Test Embeddings\n",
    "    try:\n",
    "        print(\"\\nTesting embedding generation...\")\n",
    "        \n",
    "        # Test cases that mirror our domain\n",
    "        test_cases = [\n",
    "            \"Column name standards: Use UPPER_SNAKE_CASE for all database columns\",\n",
    "            \"Data type rule: DATE columns must not allow future dates\",\n",
    "            \"Naming convention: Add _PCT suffix for percentage columns\",\n",
    "            \"Data modeling guideline: Primary keys should end with _ID\"\n",
    "        ]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = embedding_model.encode(test_cases)\n",
    "        \n",
    "        # Verify embedding dimensions\n",
    "        print(f\"\\nEmbedding dimensions:\")\n",
    "        print(f\"Number of test cases: {len(test_cases)}\")\n",
    "        print(f\"Embedding shape: {embeddings.shape}\")\n",
    "        print(f\"Vector dimension: {embeddings.shape[1]}\")  # Should be 768 for MPNet base\n",
    "        \n",
    "        # Test similarity (optional but useful)\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        print(\"\\nSimilarity matrix shape:\", similarity_matrix.shape)\n",
    "        \n",
    "        # Show example similarity\n",
    "        print(\"\\nExample similarities between first and other sentences:\")\n",
    "        for i, score in enumerate(similarity_matrix[0]):\n",
    "            if i > 0:  # Skip self-similarity\n",
    "                print(f\"Similarity with sentence {i+1}: {score:.4f}\")\n",
    "        \n",
    "        print(\"\\n✓ Embedding test completed\")\n",
    "        return embedding_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"× Error during embedding generation: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run test\n",
    "if __name__ == \"__main__\":\n",
    "    embedding_model = test_mpnet_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model successfully generates 768-dimensional embeddings and demonstrates meaningful semantic understanding of data modeling concepts. Similarity scores between sentences about naming conventions (0.4276) vs data types (0.2011) show the model can differentiate between different aspects of data modeling. This suggests MPNet will be effective for our GraphRAG implementation, particularly in retrieving relevant naming conventions and guidelines. However, we'll need to carefully structure our knowledge base to take advantage of these semantic relationships. The next step is to test this with ChromaDB and our actual guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
